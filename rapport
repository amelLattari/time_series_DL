Résumé des Étapes pour Déployer une API avec MLOps :

1. Construction et Entraînement du Modèle
-Enregistrement du modèle dans un fichier .h5 après l'entraînement.
2. Développement de l'API Flask
-Mise en place d'une API Flask pour permettre la prédiction en temps réel :
-Chargement du modèle depuis le fichier .h5.
-Création d'une route /predict qui reçoit des données en JSON et renvoie une prédiction.
-Ajout de PrometheusMetrics pour surveiller les performances de l'API (monitoring).
3. Conteneurisation avec Docker
-Création d'un fichier Dockerfile pour containeriser l'API Flask :
-Image de base Python.
-Installation des dépendances via requirements.txt.
-Exécution de l'API via Flask.
-Construction de l'image Docker locale avec la commande : "docker build -t mlops-api" .
-Test local du conteneur avec la commande : "docker run -p 5000:5000 mlops-api"
-Utilisation de curl pour tester l'API localement 
4. Orchestration avec Kubernetes
-Création d'un fichier deployment.yaml pour définir :
-Ledéploiement de l'API sur Kubernetes.
-Un service LoadBalancer pour exposer l'API.
-Application du fichier de déploiement sur Kubernetes : "kubectl apply -f deployment.yaml"
-Vérification des ressources Kubernetes :{kubectl get deployments,kubectl get services,kubectl get pods}
5. Monitoring et Logging
-Ajout de monitoring avec Prometheus :
-Configuration d’un endpoint Flask avec prometheus_flask_exporter.
-Création de règles d’alerte dans un fichier alert_rules.yml.
-Configuration de Prometheus pour scruter les métriques de l'API.
-Mise en place de logging centralisé avec la stack ELK (Elasticsearch, Logstash, Kibana) via docker-compose.

6. Tests et Validation
-Test de l'API exposée via Kubernetes en utilisant l'IP.
-Résolution des erreurs de connexion en corrigeant les configurations des pods et des services.


Déploiement du Modèle via une API
-Objectifs
Mettre à disposition un modèle de prédiction via une API RESTful.
Offrir un service capable de recevoir des données de vente en temps réel et de retourner des prédictions.
-Technologie Utilisée
Framework : Flask.
Modèle : Un modèle de prédiction entraîné en TensorFlow/Keras.
Normalisation : Scalers pour adapter les données d'entrée à la plage utilisée lors de l'entraînement.
Prometheus : Monitoring des métriques du modèle.
-Architecture
Endpoints Déployés :
/predict : Reçoit des données de ventes en entrée et retourne une prédiction.
-Fonctionnement
Entrée : Les données de vente (12 mois) sont envoyées en POST via formulaire.
Processus :
Les données sont normalisées avec des fonctions qui correspondent aux scalers utilisés pendant l'entraînement.
Le modèle effectue une prédiction basée sur les données fournies.
La prédiction est dénormalisée pour revenir à l'échelle d'origine.
Les résultats sont intégrés à une interface utilisateur.
Sortie : Une prédiction pour la prochaine période (le 13ᵉ mois).

Développement d'une Application pour Simuler et Prédire les Ventes
-Objectifs
Fournir une interface utilisateur interactive pour tester le modèle.
Permettre aux utilisateurs de visualiser les prédictions sous forme de graphiques.
-Technologie Utilisée
Framework : Flask (pour l'API et l'application frontend).
Frontend :
HTML + CSS pour l'interface utilisateur.
Matplotlib pour la visualisation des prédictions sous forme de graphiques.
-Création de l'Application
a) Interface Utilisateur
-Page d'accueil :
Un formulaire pour saisir les données de vente sur 12 mois.
Un bouton "Prédire" pour envoyer les données à l'API /predict.
-Page de Résultats :
Affichage de la prédiction pour le 13ᵉ mois.
Un graphique comparant les données réelles saisies par l'utilisateur et les prédictions du modèle.
b) Prédiction et Visualisation
Une fois les données soumises, elles sont envoyées à l'API /predict.
L'API retourne la prédiction ainsi qu'une image encodée en base64 représentant le graphique.
Le frontend affiche les résultats et le graphique.

Intégration de l’API
a) Fonctionnement
L'application frontend interagit avec l'API via HTTP.
Méthode POST : Les données de vente sont envoyées sous forme de formulaire à l'endpoint /predict.
Retour JSON : La prédiction et les graphiques sont récupérés.
